{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Seq2Seq\n",
    "\n",
    "- https://medium.com/@dev.elect.iitd/neural-machine-translation-using-word-level-seq2seq-model-47538cba8cd7\n",
    "- https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "- https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/\n",
    "- https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n",
    "- https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "- https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "- **`encoder`: it processes the input sequence and returns its own internal state.** \n",
    "\n",
    "    - encoder RNN의 아웃풋은 버린다! only recovering the state.\n",
    "    - This state will serve as the \"context\", or \"conditioning\"      \n",
    "    \n",
    "    \n",
    "- **`decoder`: it is trained to predict the next characters of the target sequence, given previous characters of the target sequence.** \n",
    "    - it is trained to **turn the target sequences into the same sequences** \n",
    "    - but offset by one timestep in the future, \n",
    "    - a training process called \"teacher forcing\" in this context. \n",
    "    \n",
    "    - `Importantly, the encoder uses as initial state the state vectors from the encoder, which is how the decoder obtains information about what it is supposed to generate.` \n",
    "    - Effectively, the decoder learns to generate targets[t+1...] given targets[...t], conditioned on the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:05:12.012553Z",
     "start_time": "2020-09-29T09:05:10.329068Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:05:12.119116Z",
     "start_time": "2020-09-29T09:05:12.014192Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  (602325, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc</th>\n",
       "      <th>story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Large tree with many outstretching branches an...</td>\n",
       "      <td>Our landmark tree in town was about to be dest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A green sign is describing a historic tree and...</td>\n",
       "      <td>So we decided to take the day to go out and en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A large tree with roots that look like crocodi...</td>\n",
       "      <td>To see the final glimpse of the roots, extendi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Big old tree being photographed on a sunny day</td>\n",
       "      <td>And its magnificent trunk, larger than life it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Huge brown tree roots rose above the ground.</td>\n",
       "      <td>One last picture of its beauty so we could cap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Large tree with many outstretching branches an...</td>\n",
       "      <td>We found this tree when we were walking in a n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A green sign is describing a historic tree and...</td>\n",
       "      <td>It turns out it is a popular attraction here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A large tree with roots that look like crocodi...</td>\n",
       "      <td>The tree is very unusual, with its roots exposed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Big old tree being photographed on a sunny day</td>\n",
       "      <td>The trunk was really wide, as much as 12 feet!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Huge brown tree roots rose above the ground.</td>\n",
       "      <td>You can see how big these roots are - pretty a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Large tree with many outstretching branches an...</td>\n",
       "      <td>Pictures of a tree are taken.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>A green sign is describing a historic tree and...</td>\n",
       "      <td>The top of the tree is taken.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A large tree with roots that look like crocodi...</td>\n",
       "      <td>Another part of the tree mostly the roots.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Big old tree being photographed on a sunny day</td>\n",
       "      <td>Some more different parts of the tree.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Huge brown tree roots rose above the ground.</td>\n",
       "      <td>And some more parts of the tree is taking.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 desc  \\\n",
       "0   Large tree with many outstretching branches an...   \n",
       "1   A green sign is describing a historic tree and...   \n",
       "2   A large tree with roots that look like crocodi...   \n",
       "3      Big old tree being photographed on a sunny day   \n",
       "4        Huge brown tree roots rose above the ground.   \n",
       "5   Large tree with many outstretching branches an...   \n",
       "6   A green sign is describing a historic tree and...   \n",
       "7   A large tree with roots that look like crocodi...   \n",
       "8      Big old tree being photographed on a sunny day   \n",
       "9        Huge brown tree roots rose above the ground.   \n",
       "10  Large tree with many outstretching branches an...   \n",
       "11  A green sign is describing a historic tree and...   \n",
       "12  A large tree with roots that look like crocodi...   \n",
       "13     Big old tree being photographed on a sunny day   \n",
       "14       Huge brown tree roots rose above the ground.   \n",
       "\n",
       "                                                story  \n",
       "0   Our landmark tree in town was about to be dest...  \n",
       "1   So we decided to take the day to go out and en...  \n",
       "2   To see the final glimpse of the roots, extendi...  \n",
       "3   And its magnificent trunk, larger than life it...  \n",
       "4   One last picture of its beauty so we could cap...  \n",
       "5   We found this tree when we were walking in a n...  \n",
       "6       It turns out it is a popular attraction here.  \n",
       "7   The tree is very unusual, with its roots exposed.  \n",
       "8      The trunk was really wide, as much as 12 feet!  \n",
       "9   You can see how big these roots are - pretty a...  \n",
       "10                      Pictures of a tree are taken.  \n",
       "11                      The top of the tree is taken.  \n",
       "12         Another part of the tree mostly the roots.  \n",
       "13             Some more different parts of the tree.  \n",
       "14         And some more parts of the tree is taking.  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load\n",
    "with open('desc_story_data_df.pkl', 'rb') as f:\n",
    "    df_story_and_desc = pickle.load(f)\n",
    "\n",
    "print (\"before: \", df_story_and_desc.shape)\n",
    "# df_story_and_desc = df_story_and_desc[:150000]\n",
    "\n",
    "desc_story_text = df_story_and_desc[['desc', 'story']]\n",
    "desc_story_text.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> 15개 단위로 같은 사진 set에 대한 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:05:12.153931Z",
     "start_time": "2020-09-29T09:05:12.121519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair 0:Large tree with many outstretching branches and leaves. ==> Our landmark tree in town was about to be destroyed and cleared for a new mall. \n",
      "pair 5:Large tree with many outstretching branches and leaves. ==> We found this tree when we were walking in a nearby town. \n",
      "pair 10:Large tree with many outstretching branches and leaves. ==> Pictures of a tree are taken.\n",
      "pair 15:Large tree with many outstretching branches and leaves. ==> They went to the botanic gardens specifically to see the large tree.\n",
      "pair 20:Large tree with many outstretching branches and leaves. ==> We went to see the largest tree in the country. \n"
     ]
    }
   ],
   "source": [
    "check_sent = \"Large tree with many outstretching branches and leaves.\"\n",
    "for i, row in desc_story_text[desc_story_text['desc'] == check_sent].iterrows():\n",
    "    print(\"pair \" + str(i) + \":\" + row['desc'] + \" ==> \" + row['story'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> 하나의 Description Text에 5개의 Story가 대응됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:05:13.290141Z",
     "start_time": "2020-09-29T09:05:13.284824Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_duplicate(df):\n",
    "    drop_df = df.drop_duplicates(\"desc\")\n",
    "    drop_df = drop_df.drop_duplicates(\"story\")\n",
    "    drop_df = drop_df.reset_index(drop=True)\n",
    "    return drop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:05:13.622737Z",
     "start_time": "2020-09-29T09:05:13.501184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39771, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_story_text = drop_duplicate(desc_story_text)\n",
    "desc_story_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess text\n",
    "\n",
    "- Add a start and end token to each sentence.\n",
    "- Clean the sentences by removing special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:05:14.547024Z",
     "start_time": "2020-09-29T09:05:14.541203Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = re.sub('[^a-zA-Z]+', ' ', w)\n",
    "    w = re.sub('[^a-zA-Z.,!?]+', ' ', w)\n",
    "    w = w.strip()\n",
    "    \n",
    "    # adding a start and an end token to the sentence\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:05:15.290271Z",
     "start_time": "2020-09-29T09:05:14.765919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_desc</th>\n",
       "      <th>out_story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; Large tree with many outstretching bra...</td>\n",
       "      <td>&lt;start&gt; Our landmark tree in town was about to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; A green sign is describing a historic ...</td>\n",
       "      <td>&lt;start&gt; So we decided to take the day to go ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; A large tree with roots that look like...</td>\n",
       "      <td>&lt;start&gt; To see the final glimpse of the roots ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; Big old tree being photographed on a s...</td>\n",
       "      <td>&lt;start&gt; And its magnificent trunk larger than ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; Huge brown tree roots rose above the g...</td>\n",
       "      <td>&lt;start&gt; One last picture of its beauty so we c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39766</th>\n",
       "      <td>&lt;start&gt; AN ADVERTISEMENT ON GLASS FOR A BREWIN...</td>\n",
       "      <td>&lt;start&gt; A group of friends visited a brewery &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39767</th>\n",
       "      <td>&lt;start&gt; A man holds the woman s hand under the...</td>\n",
       "      <td>&lt;start&gt; They were very excited &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39768</th>\n",
       "      <td>&lt;start&gt; A young man and older woman sitting do...</td>\n",
       "      <td>&lt;start&gt; They sampled many different beers &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39769</th>\n",
       "      <td>&lt;start&gt; An elderly couple dances outside of a ...</td>\n",
       "      <td>&lt;start&gt; After becoming a little buzzed they ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39770</th>\n",
       "      <td>&lt;start&gt; Man and two women standing at a counte...</td>\n",
       "      <td>&lt;start&gt; They had a great time and claimed that...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39771 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 in_desc  \\\n",
       "0      <start> Large tree with many outstretching bra...   \n",
       "1      <start> A green sign is describing a historic ...   \n",
       "2      <start> A large tree with roots that look like...   \n",
       "3      <start> Big old tree being photographed on a s...   \n",
       "4      <start> Huge brown tree roots rose above the g...   \n",
       "...                                                  ...   \n",
       "39766  <start> AN ADVERTISEMENT ON GLASS FOR A BREWIN...   \n",
       "39767  <start> A man holds the woman s hand under the...   \n",
       "39768  <start> A young man and older woman sitting do...   \n",
       "39769  <start> An elderly couple dances outside of a ...   \n",
       "39770  <start> Man and two women standing at a counte...   \n",
       "\n",
       "                                               out_story  \n",
       "0      <start> Our landmark tree in town was about to...  \n",
       "1      <start> So we decided to take the day to go ou...  \n",
       "2      <start> To see the final glimpse of the roots ...  \n",
       "3      <start> And its magnificent trunk larger than ...  \n",
       "4      <start> One last picture of its beauty so we c...  \n",
       "...                                                  ...  \n",
       "39766  <start> A group of friends visited a brewery <...  \n",
       "39767               <start> They were very excited <end>  \n",
       "39768    <start> They sampled many different beers <end>  \n",
       "39769  <start> After becoming a little buzzed they ev...  \n",
       "39770  <start> They had a great time and claimed that...  \n",
       "\n",
       "[39771 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data = desc_story_text.copy()\n",
    "clean_data['desc'] = desc_story_text['desc'].apply(lambda x: preprocess_sentence(x))\n",
    "clean_data['story'] = desc_story_text['story'].apply(lambda x: preprocess_sentence(x))\n",
    "clean_data.columns = ['in_desc','out_story']\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:05:15.358745Z",
     "start_time": "2020-09-29T09:05:15.353507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Large tree with many outstretching branches and leaves <end>\n",
      "<start> Our landmark tree in town was about to be destroyed and cleared for a new mall <end>\n"
     ]
    }
   ],
   "source": [
    "print(clean_data[\"in_desc\"][0])\n",
    "print(clean_data[\"out_story\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing and Padding\n",
    "\n",
    "- Create a word index and reverse word index (dictionaries mapping from word → id and id → word).   \n",
    ": 단어 색인과 역방향 단어 색인을 만듭니다 (단어 → ID 및 ID → 단어에서 매핑 된 사전).\n",
    "\n",
    "- Pad each sentence to a maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:05:16.387938Z",
     "start_time": "2020-09-29T09:05:16.379201Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(input_desc, output_story, flag):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(input_desc + output_story)\n",
    "        \n",
    "    if flag == \"input\":\n",
    "        # sentence 최대 길이로 패딩 자동으로 해줌\n",
    "        input_seq = tokenizer.texts_to_sequences(input_desc)\n",
    "        padding_flag = 'pre' # input은 앞에서 0을 채움\n",
    "        input_pad_seq = tf.keras.preprocessing.sequence.pad_sequences(input_seq, padding=padding_flag)\n",
    "        \n",
    "        return input_pad_seq, tokenizer\n",
    "    else:\n",
    "        output_seq = tokenizer.texts_to_sequences(output_story)\n",
    "        padding_flag = 'post' # output은 뒤에서 0을 채움\n",
    "        output_pad_seq = tf.keras.preprocessing.sequence.pad_sequences(output_seq, padding=padding_flag)\n",
    "    \n",
    "        return output_pad_seq, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:05:17.117731Z",
     "start_time": "2020-09-29T09:05:17.108773Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_token_data(data):\n",
    "    # creating cleaned input, output pairs\n",
    "    input_desc, output_story = data['in_desc'], data['out_story']\n",
    "    \n",
    "    tokenize_input_desc, tokenizer = tokenize(input_desc, output_story, \"input\")\n",
    "    tokenize_output_story, tokenizer= tokenize(input_desc, output_story, \"output\")\n",
    "    \n",
    "    input_desc_train, input_desc_test, output_story_train, output_story_test = train_test_split(tokenize_input_desc, tokenize_output_story, test_size=0.2)\n",
    "    \n",
    "    print(\"input_desc_train: \", input_desc_train.shape)\n",
    "    print(\"input_desc_test: \", input_desc_test.shape)\n",
    "    print(\"output_story_train: \", output_story_train.shape)\n",
    "    print(\"output_story_test: \", output_story_test.shape)\n",
    "    \n",
    "    return input_desc_train, input_desc_test, output_story_train, output_story_test, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:05:19.302707Z",
     "start_time": "2020-09-29T09:05:17.491358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_desc_train:  (31816, 72)\n",
      "input_desc_test:  (7955, 72)\n",
      "output_story_train:  (31816, 77)\n",
      "output_story_test:  (7955, 77)\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "\n",
    "input_desc_train, input_desc_test, output_story_train, output_story_test, tokenizer = get_token_data(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:05:19.307572Z",
     "start_time": "2020-09-29T09:05:19.304267Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  181, 8694,    5],\n",
       "       [   0,    0,    0, ...,  190,  153,    5],\n",
       "       [   0,    0,    0, ...,    2,  450,    5],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  837,  497,    5],\n",
       "       [   0,    0,    0, ...,    2,  422,    5],\n",
       "       [   0,    0,    0, ...,   72, 4761,    5]], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_desc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:05:19.315055Z",
     "start_time": "2020-09-29T09:05:19.309247Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   3,    1, 6412, ...,    0,    0,    0],\n",
       "       [   3,   38,  451, ...,    0,    0,    0],\n",
       "       [   3,  484,   17, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   3,   97,  106, ...,    0,    0,    0],\n",
       "       [   3, 3458,   29, ...,    0,    0,    0],\n",
       "       [   3,   23,   31, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_story_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:05:19.323326Z",
     "start_time": "2020-09-29T09:05:19.316620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4, 8, 41, 82, 97, 164, 194, 328, 388, 776, 3977, 7954, 15908, 31816]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_divisor(num):\n",
    "    divisors = []\n",
    "    length = int(math.sqrt(num)) + 1\n",
    "    for i in range(1, length):\n",
    "        if num % i == 0:\n",
    "            divisors.append(i)\n",
    "            divisors.append(num // i) # 나누기 연산 후 정수부분만 구하기,\n",
    "\n",
    "    divisors.sort()\n",
    "\n",
    "    return divisors\n",
    "\n",
    "get_divisor(len(input_desc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T05:16:00.985517Z",
     "start_time": "2020-10-03T05:16:00.977109Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "\n",
    "# prefetch = gpu에 올라갈 데이터 slices\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((input_desc_train, output_story_train)).shuffle(len(input_desc_train)).batch(164).prefetch(164)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((input_desc_test, output_story_test)).batch(1) # 하나씩 출력하기 위해서 batch 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the encoder and decoder model\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" width=\"500\">\n",
    "\n",
    "- FC = Fully connected (dense) layer\n",
    "- EO = Encoder output\n",
    "- H = hidden state\n",
    "- X = input to the decoder\n",
    "\n",
    "---\n",
    "\n",
    "- score = FC(tanh(FC(EO) + FC(H)))\n",
    "- attention weights = softmax(score, axis = 1). Softmax by default is applied on the last axis but here we want to apply it on - the 1st axis, since the shape of score is (batch_size, max_length, hidden_size). Max_length is the length of our input. - - - Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
    "- context vector = sum(attention weights * EO, axis = 1). Same reason as above for choosing axis as 1.\n",
    "- embedding output = The input to the decoder X is passed through an embedding layer.\n",
    "- merged vector = concat(embedding output, context vector)\n",
    "- This merged vector is then given to the GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:05:29.741920Z",
     "start_time": "2020-09-29T09:05:29.720653Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.emb = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # return_state는 return하는 Output에 최근의 state를 더해주느냐에 대한 옵션\n",
    "        # 즉, Hidden state와 Cell state를 출력해주기 위한 옵션이라고 볼 수 있다.\n",
    "        # default는 False이므로 주의하자!\n",
    "        # return_sequence=True로하는 이유는 Attention mechanism을 사용할 때 우리가 key와 value는\n",
    "        # Encoder에서 나오는 Hidden state 부분을 사용했어야 했다. 그러므로 모든 Hidden State를 사용하기 위해 바꿔준다.\n",
    "\n",
    "        self.lstm = tf.keras.layers.LSTM(512, \n",
    "                                         return_sequences=True, \n",
    "                                         return_state=True, \n",
    "                                         recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, training=False, mask=None):\n",
    "        x = self.emb(x)\n",
    "        H, h, c = self.lstm(x)\n",
    "        \n",
    "        return H, h, c\n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.emb = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        # return_sequence는 return 할 Output을 full sequence 또는 Sequence의 마지막에서 출력할지를 결정하는 옵션\n",
    "        # False는 마지막에만 출력, True는 모든 곳에서의 출력\n",
    "        self.lstm = tf.keras.layers.LSTM(512, \n",
    "                                         return_sequences=True, \n",
    "                                         return_state=True,\n",
    "                                         recurrent_initializer='glorot_uniform')\n",
    "        # LSTM 출력에다가 Attention value를 dense에 넘겨주는 것이 Attention mechanism이므로\n",
    "        self.att = tf.keras.layers.Attention()\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        # x : shifted output, s0 : Decoder단의 처음들어오는 Hidden state\n",
    "        # c0 : Decoder단의 처음들어오는 cell state H: Encoder단의 Hidden state(Key와 value로 사용)\n",
    "        x, s0, c0, H = inputs\n",
    "        x = self.emb(x)\n",
    "\n",
    "        # initial_state는 셀의 첫 번째 호출로 전달 될 초기 상태 텐서 목록을 의미\n",
    "        # 이전의 Encoder에서 만들어진 Hidden state와 Cell state를 입력으로 받아야 하므로\n",
    "        # S : Hidden state를 전부다 모아놓은 것이 될 것이다.(Query로 사용)\n",
    "        S, h, c = self.lstm(x, initial_state=[s0, c0])\n",
    "\n",
    "        # Query로 사용할 때는 하나 앞선 시점을 사용해줘야 하므로\n",
    "        # s0가 제일 앞에 입력으로 들어가는데 현재 Encoder 부분에서의 출력이 batch 크기에 따라서 length가 현재 1이기 때문에 2차원형태로 들어오게 된다.\n",
    "        # 그러므로 이제 3차원 형태로 확장해 주기 위해서 newaxis를 넣어준다.\n",
    "        # 또한 decoder의 S(Hidden state) 중에 마지막은 예측할 다음이 없으므로 배제해준다.\n",
    "        S_ = tf.concat([s0[:, tf.newaxis, :], S[:, :-1, :]], axis=1)\n",
    "\n",
    "        # Attention 적용\n",
    "        # 아래 []안에는 원래 Query, Key와 value 순으로 입력해야하는데 아래처럼 두가지만 입력한다면\n",
    "        # 마지막 것을 Key와 value로 사용한다.\n",
    "        A = self.att([S_, H])\n",
    "\n",
    "        y = tf.concat([S, A], axis=-1)\n",
    "        \n",
    "        return self.dense(y), h, c\n",
    "    \n",
    "class Seq2seq(tf.keras.Model):\n",
    "    def __init__(self, sos, eos, vocab_size, embedding_dim):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        self.enc = Encoder(vocab_size, embedding_dim)\n",
    "        self.dec = Decoder(vocab_size, embedding_dim)\n",
    "        self.sos = sos\n",
    "        self.eos = eos\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        if training is True:\n",
    "            # 학습을 하기 위해서는 우리가 입력과 출력 두가지를 다 알고 있어야 한다.\n",
    "            # 출력이 필요한 이유는 Decoder단의 입력으로 shited_ouput을 넣어주게 되어있기 때문이다.\n",
    "            x, y = inputs\n",
    "\n",
    "            # LSTM으로 구현되었기 때문에 Hidden State와 Cell State를 출력으로 내준다.\n",
    "            H, h, c = self.enc(x)\n",
    "\n",
    "            # Hidden state와 cell state, shifted output을 초기값으로 입력 받고\n",
    "            # 출력으로 나오는 y는 Decoder의 결과이기 때문에 전체 문장이 될 것이다.\n",
    "            y, _, _ = self.dec((y, h, c, H))\n",
    "            \n",
    "            return y\n",
    "\n",
    "        else:\n",
    "            x = inputs\n",
    "            H, h, c = self.enc(x)\n",
    "\n",
    "            # Decoder 단에 제일 먼저 sos를 넣어주게끔 tensor화시키고\n",
    "            y = tf.convert_to_tensor(self.sos)\n",
    "            # shape을 맞춰주기 위한 작업이다.\n",
    "            y = tf.reshape(y, (1, 1))\n",
    "\n",
    "            # 최대 64길이 까지 출력으로 받을 것이다.\n",
    "            seq = tf.TensorArray(tf.int32, 64)\n",
    "\n",
    "            # tf.keras.Model에 의해서 call 함수는 auto graph모델로 변환이 되게 되는데,\n",
    "            # 이때, tf.range를 사용해 for문이나 while문을 작성시 내부적으로 tf 함수로 되어있다면\n",
    "            # 그 for문과 while문이 굉장히 효율적으로 된다.\n",
    "            for idx in tf.range(64):\n",
    "                y, h, c = self.dec([y, h, c, H])\n",
    "                # 아래 두가지 작업은 test data를 예측하므로 처음 예측한값을 다시 다음 step의 입력으로 넣어주어야하기에 해야하는 작업이다.\n",
    "                # 위의 출력으로 나온 y는 softmax를 지나서 나온 값이므로\n",
    "                # 가장 높은 값의 index값을 tf.int32로 형변환해주고\n",
    "                # 위에서 만들어 놓았던 TensorArray에 idx에 y를 추가해준다.\n",
    "                y = tf.cast(tf.argmax(y, axis=-1), dtype=tf.int32)\n",
    "                # 위의 값을 그대로 넣어주게 되면 Dimension이 하나밖에 없어서\n",
    "                # 실제로 네트워크를 사용할 때 Batch를 고려해서 사용해야 하기 때문에 (1,1)으로 설정해 준다.\n",
    "                y = tf.reshape(y, (1, 1))\n",
    "                seq = seq.write(idx, y)\n",
    "\n",
    "                if y == self.eos:\n",
    "                    break\n",
    "            \n",
    "            # stack은 그동안 TensorArray로 받은 값을 쌓아주는 작업을 한다.    \n",
    "            return tf.reshape(seq.stack(), (1, 64))\n",
    "        \n",
    "# Implement training loop\n",
    "@tf.function\n",
    "def train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_accuracy):\n",
    "    # output_labels는 실제 output과 비교하기 위함\n",
    "    # shifted_labels는 Decoder부분에 입력을 넣기 위함\n",
    "    output_labels = labels[:, 1:]\n",
    "    shifted_labels = labels[:, :-1]\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model([inputs, shifted_labels], training=True)\n",
    "        loss = loss_object(output_labels, predictions)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(output_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T10:27:41.489869Z",
     "start_time": "2020-09-29T09:05:30.051079Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.270072102546692, Accuracy:85.521728515625\n",
      "Epoch 2, Loss: 1.0850343704223633, Accuracy:86.42794799804688\n",
      "Epoch 3, Loss: 1.0010583400726318, Accuracy:86.90519714355469\n",
      "Epoch 4, Loss: 0.9440162181854248, Accuracy:87.23162841796875\n",
      "Epoch 5, Loss: 0.9032987952232361, Accuracy:87.46558380126953\n",
      "Epoch 6, Loss: 0.8722909688949585, Accuracy:87.64449310302734\n",
      "Epoch 7, Loss: 0.8468963503837585, Accuracy:87.79431915283203\n",
      "Epoch 8, Loss: 0.8250592947006226, Accuracy:87.92412567138672\n",
      "Epoch 9, Loss: 0.805786669254303, Accuracy:88.03858947753906\n",
      "Epoch 10, Loss: 0.7882952094078064, Accuracy:88.1427230834961\n",
      "Epoch 11, Loss: 0.7720071077346802, Accuracy:88.23883819580078\n",
      "Epoch 12, Loss: 0.7565454244613647, Accuracy:88.33019256591797\n",
      "Epoch 13, Loss: 0.7417263388633728, Accuracy:88.4198989868164\n",
      "Epoch 14, Loss: 0.7274039387702942, Accuracy:88.50878143310547\n",
      "Epoch 15, Loss: 0.7134785056114197, Accuracy:88.6003189086914\n",
      "Epoch 16, Loss: 0.6999036073684692, Accuracy:88.69689178466797\n",
      "Epoch 17, Loss: 0.6866285800933838, Accuracy:88.79898071289062\n",
      "Epoch 18, Loss: 0.6736317276954651, Accuracy:88.90575408935547\n",
      "Epoch 19, Loss: 0.6608999967575073, Accuracy:89.01679229736328\n",
      "Epoch 20, Loss: 0.6484397649765015, Accuracy:89.1315689086914\n",
      "Epoch 21, Loss: 0.6362219452857971, Accuracy:89.25116729736328\n",
      "Epoch 22, Loss: 0.6242401599884033, Accuracy:89.37435150146484\n",
      "Epoch 23, Loss: 0.6124931573867798, Accuracy:89.50091552734375\n",
      "Epoch 24, Loss: 0.6009706854820251, Accuracy:89.63109588623047\n",
      "Epoch 25, Loss: 0.5896700620651245, Accuracy:89.76422119140625\n",
      "Epoch 26, Loss: 0.5785801410675049, Accuracy:89.90060424804688\n",
      "Epoch 27, Loss: 0.5676912665367126, Accuracy:90.03888702392578\n",
      "Epoch 28, Loss: 0.5570010542869568, Accuracy:90.1800537109375\n",
      "Epoch 29, Loss: 0.546516478061676, Accuracy:90.32254791259766\n",
      "Epoch 30, Loss: 0.536231279373169, Accuracy:90.46707153320312\n",
      "Epoch 31, Loss: 0.5261414647102356, Accuracy:90.6126708984375\n",
      "Epoch 32, Loss: 0.5162456035614014, Accuracy:90.75895690917969\n",
      "Epoch 33, Loss: 0.5065382719039917, Accuracy:90.90633392333984\n",
      "Epoch 34, Loss: 0.49701228737831116, Accuracy:91.05460357666016\n",
      "Epoch 35, Loss: 0.4876764714717865, Accuracy:91.20280456542969\n",
      "Epoch 36, Loss: 0.478524386882782, Accuracy:91.35089111328125\n",
      "Epoch 37, Loss: 0.4695584177970886, Accuracy:91.49905395507812\n",
      "Epoch 38, Loss: 0.4607696533203125, Accuracy:91.64678192138672\n",
      "Epoch 39, Loss: 0.45216190814971924, Accuracy:91.7936019897461\n",
      "Epoch 40, Loss: 0.4437410831451416, Accuracy:91.9392318725586\n",
      "Epoch 41, Loss: 0.4355044662952423, Accuracy:92.08354949951172\n",
      "Epoch 42, Loss: 0.4274443984031677, Accuracy:92.22650909423828\n",
      "Epoch 43, Loss: 0.41955873370170593, Accuracy:92.36786651611328\n",
      "Epoch 44, Loss: 0.4118533730506897, Accuracy:92.50731658935547\n",
      "Epoch 45, Loss: 0.4043329954147339, Accuracy:92.64439392089844\n",
      "Epoch 46, Loss: 0.3970048427581787, Accuracy:92.77886199951172\n",
      "Epoch 47, Loss: 0.38984745740890503, Accuracy:92.91108703613281\n",
      "Epoch 48, Loss: 0.3828604221343994, Accuracy:93.04096984863281\n",
      "Epoch 49, Loss: 0.37605369091033936, Accuracy:93.16797637939453\n",
      "Epoch 50, Loss: 0.3694174587726593, Accuracy:93.29242706298828\n"
     ]
    }
   ],
   "source": [
    "model = Seq2seq(sos = tokenizer.word_index['<start>'],\n",
    "                eos = tokenizer.word_index['<end>'],\n",
    "                vocab_size = len(tokenizer.word_index) + 1,\n",
    "                embedding_dim = 300)\n",
    "\n",
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "# 성능 지표 정의\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for seqs, labels in train_ds:\n",
    "        train_step(model, seqs, labels, loss_func, optimizer, train_loss, train_accuracy)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy:{}'\n",
    "    print(template.format(epoch + 1, train_loss.result(), train_accuracy.result() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T05:13:10.805563Z",
     "start_time": "2020-10-03T05:13:10.584529Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"seq2seq_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T05:07:47.110167Z",
     "start_time": "2020-10-03T05:07:47.104984Z"
    }
   },
   "outputs": [],
   "source": [
    "# Implement algorithm test\n",
    "@tf.function\n",
    "def test_step(model, inputs):\n",
    "    return model(inputs, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T05:16:09.833751Z",
     "start_time": "2020-10-03T05:16:08.178507Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "desc:  ['<start> the sky during a beautiful sunset near the beach <end>']\n",
      "story:  ['<start> until the sun started to go down <end>']\n",
      "pred:  ['we decided to take a trip to the beach and the beach was lit up <end>']\n",
      "_\n",
      "desc:  ['<start> a group of people wearing masks are celebrating marde gras in a bus <end>']\n",
      "story:  ['<start> halloween was fun this year my family dressed up as astrunauts <end>']\n",
      "pred:  ['the city s game was getting great to a great day in the town <end>']\n",
      "_\n",
      "desc:  ['<start> a child stretching and smiling at the camera <end>']\n",
      "story:  ['<start> today i went to a history museum with grandpa dad and sister <end>']\n",
      "pred:  ['but mom and dad you see you think they were having a great time <end>']\n",
      "_\n",
      "desc:  ['<start> two men and a woman posing for a picture <end>']\n",
      "story:  ['<start> the group decided to head out to a bar for the night <end>']\n",
      "pred:  ['we even got to see a couple in town they were a extremely proud <end>']\n",
      "_\n",
      "desc:  ['<start> two women sharing and doing cheers with drinks <end>']\n",
      "story:  ['<start> any way they choose to cook it a toast for the holiday is in order <end>']\n",
      "pred:  ['the night started out and started off drinking onto the night <end>']\n",
      "_\n",
      "desc:  ['<start> the front visage of an old and imposing cathedral <end>']\n",
      "story:  ['<start> the old city was simply breathtaking <end>']\n",
      "pred:  ['we were excited to finally get to the downtown museum <end>']\n",
      "_\n",
      "desc:  ['<start> a plate with eggs toast and bananas on it next to a glass of milk and a mug of coffee <end>']\n",
      "story:  ['<start> breakfast of champions before heading out the door <end>']\n",
      "pred:  ['they make their own snack and look beautiful while they were <end>']\n",
      "_\n",
      "desc:  ['<start> a black and white dog looking at a plush dog <end>']\n",
      "story:  ['<start> then i played with m dog in the living room <end>']\n",
      "pred:  ['the dog was very happy <end>']\n",
      "_\n",
      "desc:  ['<start> the fried chicken is on a white plate <end>']\n",
      "story:  ['<start> my boss took a sample of the fish to see if he approved of the taste <end>']\n",
      "pred:  ['we have so much food <end>']\n",
      "_\n",
      "desc:  ['<start> luscious tropical vegetation surrounds an ornate fountain in front of a vibrant blue sky <end>']\n",
      "story:  ['<start> this was the view from the front of the shanty a beautiful fountain and wild flowers growing everywhere you looked beyond this beauty was the calm water of cove lake i smiled and sighed heavily john knew it was a good thing <end>']\n",
      "pred:  ['the museum had large red brick <end>']\n",
      "_\n",
      "desc:  ['<start> an outdoor playground with adults and children is in a shaded area <end>']\n",
      "story:  ['<start> they went on the merrygoround <end>']\n",
      "pred:  ['we all took a family together <end>']\n",
      "_\n",
      "desc:  ['<start> three teen girls sit together and use a computer <end>']\n",
      "story:  ['<start> i went to class today <end>']\n",
      "pred:  ['my my sister and i bought a last photo of a kid shirt <end>']\n",
      "_\n",
      "desc:  ['<start> they posed in front of the large table <end>']\n",
      "story:  ['<start> we had a good team preparing the speech <end>']\n",
      "pred:  ['it was a great success and a beautiful hot year <end>']\n",
      "_\n",
      "desc:  ['<start> a single yellow fire work with blue streaks coming off it in the night sky <end>']\n",
      "story:  ['<start> i had a great time taking pictures of all of them <end>']\n",
      "pred:  ['at home we had a bunch of people there <end>']\n",
      "_\n",
      "desc:  ['<start> a birthday meal of sushi and other healthy food sits in a platter <end>']\n",
      "story:  ['<start> there was plenty of delicious food <end>']\n",
      "pred:  ['the meal was a huge and elegant for the meal <end>']\n",
      "_\n",
      "desc:  ['<start> the man is standing with the girl in the fake mustache <end>']\n",
      "story:  ['<start> the partygoers were all dressed up some as normal couples <end>']\n",
      "pred:  ['brad really wanted to show her in a new outfit <end>']\n",
      "_\n",
      "desc:  ['<start> the eggs are painted with black and white floral designs <end>']\n",
      "story:  ['<start> we even saw ones that made us want to sing <end>']\n",
      "pred:  ['our favorite place had food and a lot of eggs <end>']\n",
      "_\n",
      "desc:  ['<start> a desly populated area with little vegetation adn mountains in the background <end>']\n",
      "story:  ['<start> today on vacation we visited the old ruins again <end>']\n",
      "pred:  ['it was a day for a ride <end>']\n",
      "_\n",
      "desc:  ['<start> a woman that is smiling in the kitchen <end>']\n",
      "story:  ['<start> the lady brought the food to the table <end>']\n",
      "pred:  ['mom finally made it to the christmas party <end>']\n",
      "_\n",
      "desc:  ['<start> multitude of people wearing name tags gathered indoors <end>']\n",
      "story:  ['<start> the group left informed <end>']\n",
      "pred:  ['i was almost late <end>']\n",
      "_\n",
      "desc:  ['<start> a man poses for the camera with a green mask with black hair over the top of his head <end>']\n",
      "story:  ['<start> there were a lot of super heroes at the house <end>']\n",
      "pred:  ['there was a masquerade party at the park last year <end>']\n"
     ]
    }
   ],
   "source": [
    "for idx, (test_seq, test_labels) in enumerate(test_ds):\n",
    "    if idx > 20:\n",
    "        break\n",
    "    prediction = test_step(model, test_seq)\n",
    "    test_text = tokenizer.sequences_to_texts(test_seq.numpy())\n",
    "    # ground_truth\n",
    "    gt_text = tokenizer.sequences_to_texts(test_labels.numpy())\n",
    "    # prediction\n",
    "    texts = tokenizer.sequences_to_texts(prediction.numpy())\n",
    "    \n",
    "    print('_')\n",
    "    print('desc: ', test_text)\n",
    "    print('story: ', gt_text)\n",
    "    print('pred: ', texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
